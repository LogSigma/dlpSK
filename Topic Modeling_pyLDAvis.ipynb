{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토픽 모델링\n",
    "## 토픽 모델링이란?\n",
    "우리는 앞서 의미적으로 관련이 있는 문서들을 몇 개의 집단으로 묶는 과제인 문서 군집에 대하여 알아보았다. 문서 군집은 매우 유용한 기법임에 틀림없지만 중요한 한계가 한 가지 있다. 그것은 많은 경우에 한 문서는 여러 가지 내용을 가짐으로 해서 여러 집단에 속할 수 있다는 것이다. 예를 들어, 파이썬 언어로 웹 응용 프로그램을 구현하는 내용의 문서는 파이썬 관련 문서들과 묶일 수도 있고 웹 응용프로그램 관련 문서들과 엮일 수도 있다.\n",
    "\n",
    "이와 같은 상황에서 빛을 발하는 기법이 토픽 모델링이다. 토픽 모델링은 문서 군집에서처럼 문서들을 완전히 분리된 집단으로 나누는 것이 아니라 문서들과 몇 개의 토픽(화제)들을 연계한다. 물론 이 토픽들은 문서들로부터 학습에 의해 자동으로 생성된다. 이 때 특정 문서에 해당하는 토픽들은 문서의 특성에 따라 그 연계 정도, 혹은 중요도가 다르게 매겨진다. 토픽 모델링을 통해 우리는 문서 집합 내에 감추어진 화제, 혹은 정리된 개념들을 추출할 수 있으며 나아가 토픽들로 구성된 문서의 주제도 추론할 수 있다.\n",
    "\n",
    "## 토픽 모델링의 원리\n",
    "토픽 모델링에서는 잠재 디리클레 할당(Latent Dirichlet Allocation, LDA)이라는 확률 모형을 사용한다. 이 모형은 매우 복잡한 수학적 기반을 지니고 있어서 수리적 이해는 쉽지 않다. 따라서 다음과 같이 개략적으로 설명해 보자.\n",
    "\n",
    "토픽 모델링에서는 사람이 글을 생성하는 과정을 모델링한다. 즉, 사람이 어떤 대주제를 가진 글을 쓸 때에는 이와 관련하여 좀 더 구체적인 토픽(화제)들을 설정하고 이 토픽들을 나타낼 수 있는 어휘들을 적절히 배치하여 글을 작성해 간다고 보는 것이다. 역으로 텍스트로부터 어휘들이 배치된 형태를 파악한다면 토픽들을 드러낼 수 있고 이로부터 텍스트의 주제도 추론이 가능할 것이라는 것이 토픽 모델링의 논리이다.\n",
    "\n",
    "다음은 토픽 모델링의 예를 설명할 때에 많이 이용되는 그림이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![토픽 모델링의 예](figs/tm-result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 그림에서 주의할 것은 \"Arts\", \"Budgets\", \"Children\", \"Education\"으로 주어진 토픽의 명칭은 컴퓨터가 도출한 것이 아니라 사람의 판단에 의해 부여된 것이다. 컴퓨터는 구분이 되는 토픽들을 추론하여 도출할 뿐이지 그 명칭을 붙이지는 않는다.\n",
    "그림의 어휘 목록을 잘 살펴보면 '오페라(opera)'나 '음악(music)' 같은 단어들은 '예술(Arts)'을 반영할 확률이 가장 높고, '재단(foundation)'이나 '이사회(board)'는 '예산(Budgets)', '젊은(young)'이나 '기회(opportunity)'는 '아동(Children', '학교(school)'나 '가르치다(taught)'는 '교육(Education)'을 반영할 확률이 가장 높다고 분석된 것을 알 수 있다. 이러한 의미적 정보는 LDA를 이용하여 통계적으로 추론된 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토픽 모델링의 절차\n",
    "### 토픽 모델링 간단 예제\n",
    "\n",
    "우리에게 다음과 같은 문장들이 주었다고 가정하자.\n",
    "\n",
    "1. 나는 **물고기**와 **채소**를 **먹는다**.\n",
    "1. _물고기_는 _반려동물_이다.\n",
    "1. _고양이_가 **물고기**를 **먹는다**.\n",
    "\n",
    "위의 문장들에서 강조된 단어들을 두 개의 토픽에 할당할 수 있다. 즉, 굵은 글씨로 강조된 단어들은 \"음식\" 토픽, 기울인 글씨로 강조된 단어들은 \"반려동물\" 토픽에 할당할 수 있다.\n",
    "\n",
    "이와 같이 단어 수준에서 토픽을 정의하면 다음과 같은 이점이 있다.\n",
    "\n",
    "1. 문장별 토픽의 분포를 어휘 빈도로 나타낼 수 있다. 위의 예에서 문장 1은 \"음식\" 토픽 100%로 구성되어 있으며, 문장 2는 \"반려동물\" 토픽 100%로 구성되어 있다. 한편 문장 3은 \"반려동물\" 토픽 33%, \"음식\" 토픽 67%로 구성되어 있다.\n",
    "1. 개별 토픽의 내용을 해당 토픽을 구성하는 단어들의 비율을 이용해 계량적으로 나타낼 수 있다. 위의 예에서 \"음식\" 토픽은 '먹는다' 40%, '물고기' 40%, '채소 '20%로 구성된다고 보일 수 있다.\n",
    "\n",
    "### 단순화한 토픽 모델링의 절차\n",
    "이상과 같은 토픽 모델링의 절차를 간략히 설명하기 위해 다음과 같이 두 개의 문서가 주어졌다고 가정하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| T | 문서 X | T | 문서 Y |\n",
    "|:-:|:------:|:-:|:------:|\n",
    "| F | 물고기 | ? | 물고기 |\n",
    "| F | 물고기 | F | 물고기 |\n",
    "| F | 먹는다 | F | 우유   |\n",
    "| F | 먹는다 | P | 고양이 |\n",
    "| F | 채소   | P | 고양이 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**점검 대상 단어가 각각의 토픽들에서 차지하는 비율이 얼마나 되는가?** 두 문서에 모두 속한 단어 '물고기'는 토픽 F의 거의 반을 차지하지만 토픽 P에는 전혀 속하지 않고 있다. 그러므로 임의로 취한 단어 '물고기'는 토픽 F에 속할 가능성이 높다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| T | 문서 X | T | 문서 Y |\n",
    "|:-:|:------:|:-:|:------:|\n",
    "| **F** | **물고기** | ? | 물고기 |\n",
    "| **F** | **물고기** | **F** | **물고기** |\n",
    "| F | 먹는다 | F | 우유   |\n",
    "| F | 먹는다 | P | 고양이 |\n",
    "| F | 채소   | P | 고양이 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**점검 대상 단어가 속한 문서에서 각각의 토픽들이 차지하는 비율이 얼마나 되는가?** 문서 Y에 속한 단어들이 50대 50 비율로 토픽 F와 토픽 P에 할당되어 있으므로 점검 대상 단어 '물고기'가 두 토픽에 할당될 가능성은 동일하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 기준을 종합해 볼 때 문서 Y에 속한 점검 대상 단어 '물고기'는 토픽 F에 할당하는 것이 적합하다는 결론을 내릴 수 있다.\n",
    "\n",
    "LDA를 이용하면 위와 같은 토픽 할당 점검을 모든 문서에 속한 모든 단어들에 대하여 반복적으로 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">위의 설명은 <https://algobeans.com/2015/06/21/laymans-explanation-of-topic-modeling-with-lda-2>에서 제공하는 튜토리얼 문서를 우리말로 요약하여 옮긴 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토픽 모델링의 수리적 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 보인 토픽 모델링 절차를 간단히 수리적으로 표현해 보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(Z|W,D) = \n",
    "\\frac{\\text{토픽 $Z$에 속한 단어 $W$의 수}+\\beta_w}{\\text{토픽 $Z$에 속한 단어 수 총합}+\\beta} \\times (\\text{토픽 $Z$에 속한 문서 $D$ 소속 단어 수} + \\alpha)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![토픽 모델링 다이어그램](figs/topic_modeling-diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 곱셈의 첫 번째 항은 단어 $W$가 토픽 $Z$에서의 비율이 얼마나 되는가를 측정하는 것이고 두 번째 항은 단어 $W$가 속한 문서 $D$에서 토픽 $Z$가 차지하는 비율이 얼마나 되는가를 측정하는 것이다. $\\alpha$와 $\\beta$는 하이퍼파라미터라고 부르는 계수로 모델 할당을 다소 느슨하게 만들어 준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">위의 식은 <https://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/>에서 가져온 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한편 Blei (2012)에서는 다음과 같은 토픽 모델링 식을 제시한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(\\beta_{1:K}, \\theta_{1:D}, z_{1:d}, w_{1:D})\\\\\\\\\n",
    "=\\Pi_{i=1}^K P(\\beta_i) \\Pi_{d=1}^D P(\\theta_d) \n",
    "\\big(\\Pi_{n=1}^N P(z_{d,n} | \\theta_d) P(w_{d,n} | \\beta_{1:K}, z_{d,n}) \\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{aligned} \\beta_{1:K} & \\text{토픽들}\\\\\\\\ \\theta_{1:D} & \\text{문서별 토픽 비율}\\\\\\\\ z_{1:d} & \\text{문서별 토픽 할당}\\\\\\\\ w_{1:d} & \\text{문서별 단어}\\\\\\\\           \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 식을 살펴보면 토픽 할당 $z_{d,n}$은 문서별 토픽 비율인 $\\theta_d$에 의존하며, 관찰된 단어 $w_{d,n}$은 토픽 할당 $z_{d,n}$과 전체 토픽 $\\beta_{1:K}$에 의존한다. 이 의존성에 근거하여 변수들의 결합 확률 분포가 디리클레 분포라는 특정한 분포에 따른다는 가정을 하게 된다. 여기서 직접 관찰 가능한 변수는 $w_{1:d}$밖에 없는데, 이를 조건으로로 삼아 다음과 같은 조건부 확률을 구하는 식을 만든다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(\\beta_{1:K}, \\theta_{1:D}, z_{1:d} | w_{1:D})\\\\\\\\\n",
    "= \\frac{P(\\beta_{1:K}, \\theta_{1:D}, z_{1:d}, w_{1:D})}{P(w_{1:D})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 확률을 계산하려면 가능한 모든 토픽 할당 경우의 수를 생성하여야 한다. 그런데 실제로는 그럴 수 없기 때문에 반복적으로 표본 추출을 하여 확률값을 추정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![토픽 모델링의 예](figs/topic-word.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">위에서 사용하는 표본 추출 기법은 깁스 표본(Gibbs sampling) 기법이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![깁스 샘플링의 예](figs/gibbs_sampling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그렇다면, LDA의 추론과정을 어떻게 해석할 수 있을까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Dirichlet Distribution의 의미와 파라미터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![디리클레 분포 파라미터](figs/dirichlet_parameter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Topic의 갯수, K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![perplexity](figs/perplexity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference :\n",
    "1. https://ratsgo.github.io/statistics/2017/05/31/gibbs/\n",
    "2. https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/06/01/LDA/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토픽 모델링에는 gensim 라이브러리를 사용한다.  다음은 네이버 블로그를 크롤링한 텍스트에 토픽 모델링을 적용한 뒤 추출된 10개의 토픽별로 가장 확률이 높은 어휘 30개씩을 출력하는 과정이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'konlpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b48b4b64c0fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msquareform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMecab\u001b[0m \u001b[0;31m#Komoran #Mecab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'konlpy'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from operator import itemgetter\n",
    "from itertools import combinations\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.spatial.distance import squareform\n",
    "from tqdm import tqdm_notebook\n",
    "from konlpy.tag import Mecab #Komoran #Mecab\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import warnings\n",
    "import networkx as nx\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어들을 추가합니다. 한국어불용어 100개와 추가하고싶은 단어들을 추가합니다. 기본적으로 공백과, 특수문자들을 제거했습니다.\n",
    "\n",
    "SW = set()\n",
    "SW.add(\" \")\n",
    "SW.add(\"○\")\n",
    "SW.add(\"있는\")\n",
    "SW.add(\"국립중앙박물관\")\n",
    "\n",
    "for i in string.punctuation:\n",
    "    SW.add(i)\n",
    "\n",
    "with open(\"data/한국어불용어100.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        stop_word = line.split()[0]\n",
    "        SW.add(stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopword(불용어)를 확인합니다.\n",
    "SW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 15\n",
    "NUM_TOPIC_WORDS = 40\n",
    "import csv\n",
    "\n",
    "def read_documents(input_file_name):\n",
    "    \"\"\"문서들을 주어진 이름의 파일로부터 읽어들여 돌려준다.\"\"\"\n",
    "    \n",
    "    corpus = []\n",
    "\n",
    "    with open(input_file_name, \"r\", encoding=\"utf-8\") as input_file:\n",
    "        for line in input_file:\n",
    "            #line 하나가 문서 하나를 의미.\n",
    "            corpus.append(line)\n",
    "\n",
    "    return corpus\n",
    "\n",
    "\n",
    "\n",
    "def text_tokenizing(corpus, tokenizer):\n",
    "    \n",
    "    if tokenizer == \"noun\":\n",
    "        mecab = Mecab()\n",
    "        #kkma = KKma()\n",
    "        #okt = Okt()\n",
    "\n",
    "        token_corpus = []\n",
    "\n",
    "        for n in tqdm_notebook(range(len(corpus)), desc=\"Preprocessing\"):\n",
    "            token_text = mecab.nouns(corpus[n])\n",
    "            token_text = [word for word in token_text if word not in SW and len(word) > 1]\n",
    "                \n",
    "            token_corpus.append(token_text)\n",
    "\n",
    "        #print(token_corpus[:5])\n",
    "        \n",
    "    elif tokenizer == \"word\":\n",
    "        token_corpus = []\n",
    "\n",
    "        for n in tqdm_notebook(range(len(corpus)), desc=\"Preprocessing\"):\n",
    "            token_text = corpus[n].split()\n",
    "            token_text = [word for word in token_text if word not in SW and len(word) > 1]\n",
    "            token_corpus.append(token_text)\n",
    "\n",
    "        #print(token_corpus[:5])\n",
    "        \n",
    "\n",
    "    return token_corpus\n",
    "\n",
    "input_file_name = \"data/blog_text_국립중앙박물관(1-500).txt\"\n",
    "corpus = read_documents(input_file_name)\n",
    "documents = text_tokenizing(corpus, tokenizer=\"noun\") #tokenizer= \"noun\" or \"word\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문서 읽기의 과정은 앞서 단어 임베딩의 경우와 다르지 않다. 다음 과정은 문서-단어 행렬을 만드는 과정이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서-단어 행렬 만들기\n",
    "# 어휘(vocabulary) 학습\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "# 문서-단어 행렬 생성\n",
    "corpus = [dictionary.doc2bow(document) for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    with open(\"dictionary.pk\", \"wb\") as f:\n",
    "        pickle.dump(dictionary, f)\n",
    "\n",
    "    with open(\"corpus.pk\", 'wb') as f:\n",
    "        pickle.dump(corpus, f)\n",
    "    \n",
    "\n",
    "    with open(\"dictionary.pk\", 'rb') as f:\n",
    "        dictionary = pickle.load(f)\n",
    "\n",
    "    with open(\"corpus.pk\", 'rb') as f:\n",
    "        corpus = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 과정은 scikit-learn 모듈을 이용하는 것과 마찬가지의 과정이다. 즉, `doc2bow()` 메소드가 벡터라이저이다. 학습된 문서 집합의 어휘는 단어와 그에 해당하는 인덱스가 매핑된 구조이다. 이때 단어의 순서는 사전순이 아니라 빈도 역순이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문서-어휘 행렬은 scikit-learn에서와 같이 좌표 리스트로 나타낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF를 이용한 문서-단어 행렬을 얻고자 하면 아래와 같은 절차를 거쳐야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF 문서-단어 행렬 생성\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "#tfidf = \n",
    "corpus_tfidf = tfidf[corpus]\n",
    "corpus_tfidf[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 다시 돌아와서 토픽 모델링을 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.ldamodel.LdaModel(corpus, num_topics=NUM_TOPICS, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 인자들은 기본적인 것들이며 몇 가지 초파라미터가 더 있다.\n",
    "\n",
    "생성된 모델을 이용하여 만들어진 토피들을 살펴볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_id = dictionary.token2id[\"박물관\"]\n",
    "model.get_term_topics(word_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_topic_terms(2, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.id2token[623]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.show_topic(5, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 보인 코드대로 토픽 모델링을 수행한 뒤 토픽별로 확률이 높은 토픽 단어 30개씩을 출력하는 스크립트를 아래에 보인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NUM_TOPICS = 8\n",
    "\n",
    "NUM_TOPIC_WORDS = 30\n",
    "\n",
    "\n",
    "def read_documents(input_file_name):\n",
    "    \"\"\"문서들을 주어진 이름의 파일로부터 읽어들여 돌려준다.\"\"\"\n",
    "    \n",
    "    corpus = []\n",
    "\n",
    "    with open(input_file_name, \"r\", encoding=\"utf-8\") as input_file:\n",
    "        for line in input_file:\n",
    "            #line 하나가 문서 하나를 의미.\n",
    "            corpus.append(line)\n",
    "\n",
    "    return corpus\n",
    "\n",
    "\n",
    "\n",
    "def text_tokenizing(corpus, tokenizer):\n",
    "    \n",
    "    if tokenizer == \"noun\":\n",
    "        mecab = Mecab()\n",
    "\n",
    "        token_corpus = []\n",
    "\n",
    "        for n in tqdm_notebook(range(len(corpus)), desc=\"Preprocessing\"):\n",
    "            token_text = mecab.nouns(corpus[n])\n",
    "            token_text = [word for word in token_text if word not in SW and len(word) > 1]\n",
    "            token_corpus.append(token_text)\n",
    "\n",
    "        #print(token_corpus[:5])\n",
    "        \n",
    "    elif tokenizer == \"word\":\n",
    "        token_corpus = []\n",
    "\n",
    "        for n in tqdm_notebook(range(len(corpus)), desc=\"Preprocessing\"):\n",
    "            token_text = corpus[n].split()\n",
    "            token_text = [word for word in token_text if word not in SW and len(word) > 1]\n",
    "            \n",
    "            token_corpus.append(token_text)\n",
    "\n",
    "        #print(token_corpus[:5])\n",
    "        \n",
    "\n",
    "    return token_corpus\n",
    "\n",
    "def extract_text(token_corpus, output_file_path):\n",
    "    \n",
    "    with open(output_file_path, 'w') as f:\n",
    "        for blog in token_corpus:\n",
    "            temp_text = \"\"\n",
    "            for token in blog:\n",
    "                temp_text = temp_text + \" \" + token\n",
    "                temp_text = re.sub('[a-zA-Z]', '', temp_text)\n",
    "                temp_text = re.sub('[\\{\\}\\[\\]\\/?.,:;|\\)*~`!^\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\")]', '', temp_text)\n",
    "\n",
    "\n",
    "            print(\"%s\\n\" % temp_text, file=f)\n",
    "\n",
    "\n",
    "def build_doc_term_mat(documents):\n",
    "    \"\"\"주어진 문서 집합으로 문서-어휘 행렬을 만들어 돌려준다.\"\"\"\n",
    "    \n",
    "    print_log_msg(\"Building document-term matrix.\")\n",
    "    dictionary = corpora.Dictionary(documents)\n",
    "    corpus = [dictionary.doc2bow(document) for document in documents]\n",
    "\n",
    "    return corpus, dictionary\n",
    "\n",
    "\n",
    "def print_topic_words(model):\n",
    "    \"\"\"토픽별 토픽 단어들을 화면에 인쇄한다.\"\"\"\n",
    "    \n",
    "    print_log_msg(\"Printing topic words.\")\n",
    "    \n",
    "    for topic_id in range(model.num_topics):\n",
    "        topic_word_probs = model.show_topic(topic_id, NUM_TOPIC_WORDS)\n",
    "        print(\"Topic ID: {}\".format(topic_id))\n",
    "\n",
    "        for topic_word, prob in topic_word_probs:\n",
    "            print(\"\\t{}\\t{}\".format(topic_word, prob))\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "def print_log_msg(msg):\n",
    "    \"\"\"로그 메시지를 출력한다.\"\"\"\n",
    "    \n",
    "    print(msg, flush=True)\n",
    "    \n",
    "        \n",
    "def main():\n",
    "    \n",
    "    input_file_name = \"data/blog_text_국립중앙박물관(1-500).txt\"\n",
    "    documents = read_documents(input_file_name)\n",
    "    tokenized_documents = text_tokenizing(documents, tokenizer=\"noun\") #tokenizer= \"noun\" or \"word\"\n",
    "    extract_text(tokenized_documents, \"token_corpus.txt\")\n",
    "    corpus, dictionary = build_doc_term_mat(tokenized_documents)\n",
    "    model = models.ldamodel.LdaModel(corpus, num_topics=NUM_TOPICS,\n",
    "                                     id2word=dictionary,\n",
    "                                     alpha=1)\n",
    "    print_topic_words(model)\n",
    "\n",
    "#\n",
    "# 실행\n",
    "#\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 스크립트를 실행하면 다음과 같이 토픽별로 할당한 토픽 단어들과 해당 확률이 표시된다.\n",
    "\n",
    "```\n",
    "Topic ID: 0\n",
    "\t있는\t0.004373996518552303\n",
    "\t국립중앙박물관\t0.002882609376683831\n",
    "\t박물관\t0.0017532209167256951\n",
    "\t너무\t0.001714710728265345\n",
    "\t많이\t0.001435010344721377\n",
    "\t대한\t0.00141372240614146\n",
    "\t정말\t0.0012980009196326137\n",
    "\t함께\t0.0012935514096170664\n",
    "\t보고\t0.0012449593050405383\n",
    "\t좋은\t0.0010815684217959642\n",
    "\t많은\t0.0008918592357076705\n",
    "\t만든\t0.00083119299961254\n",
    "    ...\n",
    "```\n",
    "\n",
    "토픽의 개수를 정하는 일반적인 방법은 없다. 토픽의 갯수가 너무 많거나 적으면 변별력 있는 토픽이 제대로 추출되지 않을 것이므로 반복 실험을 통해 목적에 맞는 토픽 수를 정해야 한다. 또한 토픽 모델링은 난수 요소를 포함하고 있기 때문에 실행할 때마다 조금씩 다른 결과를 생성한다. 그러므로 반복 실행을 통해 여러 번 생성되는 토픽을 골라서 최종적으로 선택해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어휘 네트워크를 이용한 토픽 모델의 시각화\n",
    "* 위 스크립트를 앞선 강의에서 살펴본 어휘 공기 네트워크 분석 기법과 결합하면 토픽들을 관통하는 키워드들의 연결 구조를 살펴볼 수 있다.\n",
    "* 토픽모델링은 텍스트 자체를 사용하므로, word embedding된 벡터를 사용할 수 없다.\n",
    "* 그 방법으로 제시된 방법이 LDA2Vec가 있으나, 방식을 이해하기 힘드므로, 일단은 co-occurence를 이용하여 semantic network를 구성하여 본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 .pyLDAvis 모듈을 이용한 토픽 모델의 시각화\n",
    "pyLDAvis는 토픽 모델의 시각화를 위한 R 패키지인 LDAvis를 파이썬용으로 이식한 모듈이다. 이 모듈은 아나콘다 파이썬 배포판에 포함되어 있지 않으므로 Anaconda Prompt에서 `pip`으로 설치해야 한다.\n",
    "\n",
    "```\n",
    "> pip install pyldavis\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 문서별 토픽의 분포\n",
    "한편 앞서 설명한 바와 같이 한 개의 문서는 복수 개의 토픽으로 구성되므로 문서별 토픽의 분포를 얻는 것도 가능하다.\n",
    "\n",
    "위 스크립트의 실행 결과를 이용하여 문서 군집화를 수행하는 등의 추가 분석도 가능하다. 그 클러스터링 결과가 형태소를 자질로 직접 이용한 군집화 결과와 얼마나 차이가 있는지는 검증이 필요하다.\n",
    "\n",
    "군집화를 문서가 아니라 추출된 토픽들에 적용하여 토픽의 군집화를 시도하는 것도 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import ujson\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from operator import itemgetter\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "import networkx as nx\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "NUM_TOPICS = 4\n",
    "NUM_TOPIC_WORDS = 30\n",
    "NUM_WORD_COOCS = 50\n",
    "\n",
    "\n",
    "def read_documents(input_file_name):\n",
    "    \"\"\"문서들을 주어진 이름의 파일로부터 읽어들여 돌려준다.\"\"\"\n",
    "    \n",
    "    corpus = []\n",
    "\n",
    "    with open(input_file_name, \"r\", encoding=\"utf-8\") as input_file:\n",
    "        for line in input_file:\n",
    "            #line 하나가 문서 하나를 의미.\n",
    "            corpus.append(line)\n",
    "\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def text_tokenizing(corpus, tokenizer):\n",
    "    \n",
    "    if tokenizer == \"noun\":\n",
    "        mecab = Mecab()\n",
    "\n",
    "        token_corpus = []\n",
    "\n",
    "        for n in tqdm_notebook(range(len(corpus)), desc=\"Preprocessing\"):\n",
    "            token_text = mecab.nouns(corpus[n])\n",
    "            token_text = [word for word in token_text if word not in SW and len(word) > 1]\n",
    "            token_corpus.append(token_text)\n",
    "\n",
    "        #print(token_corpus[:5])\n",
    "        \n",
    "    elif tokenizer == \"word\":\n",
    "        token_corpus = []\n",
    "\n",
    "        for n in tqdm_notebook(range(len(corpus)), desc=\"Preprocessing\"):\n",
    "            token_text = corpus[n].split()\n",
    "            token_text = [word for word in token_text if word not in SW and len(word) > 1]\n",
    "            token_corpus.append(token_text)\n",
    "\n",
    "        #print(token_corpus[:5])\n",
    "        \n",
    "\n",
    "    return token_corpus\n",
    "\n",
    "def build_doc_term_mat(documents):\n",
    "    \"\"\"주어진 문서 집합으로 문서-어휘 행렬을 만들어 돌려준다.\"\"\"\n",
    "    \n",
    "    print_log_msg(\"Building document-term matrix.\")\n",
    "    dictionary = corpora.Dictionary(documents)\n",
    "    corpus = [dictionary.doc2bow(document) for document in documents]\n",
    "\n",
    "    return corpus, dictionary\n",
    "\n",
    "\n",
    "def build_word_cooc_mat(model):\n",
    "    \"\"\"주어진 토픽 모델링 결과에서 어휘 공기 행렬을 생성하여 돌려준다.\"\"\"\n",
    "    \n",
    "    print_log_msg(\"Building topic word co-occurrence matrix.\")\n",
    "    word_cooc_mat = defaultdict(Counter)\n",
    "    topic_documents = get_topic_documents(model)\n",
    "    \n",
    "    for topic_document in topic_documents:\n",
    "        for word1, word2 in combinations(topic_document, 2):\n",
    "            word_cooc_mat[word1][word2] += 1\n",
    "            \n",
    "    return word_cooc_mat\n",
    "    \n",
    "    \n",
    "def get_topic_documents(model):\n",
    "    \"\"\"주어진 토픽 모델링 결과에서 토픽 문서를 생성하여 돌려준다.\"\"\"\n",
    "    \n",
    "    print_log_msg(\"Generating topic word documents.\")\n",
    "    topic_documents = []\n",
    "    \n",
    "    for topic_id in range(model.num_topics):\n",
    "        topic_document = []\n",
    "        topic_word_probs = model.show_topic(topic_id, NUM_TOPIC_WORDS)\n",
    "\n",
    "        for topic_word, prob in topic_word_probs:\n",
    "            topic_document.append(topic_word)\n",
    "            \n",
    "        topic_documents.append(topic_document)\n",
    "        \n",
    "    return topic_documents\n",
    "\n",
    "\n",
    "def build_word_cooc_network(sorted_word_coocs):\n",
    "    \"\"\"토픽 단어 공기 네트워크를 생성하여 돌려준다.\"\"\"\n",
    "    \n",
    "    print_log_msg(\"Generating topic word co-occurrence network.\")\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for word1, word2, count in sorted_word_coocs[:NUM_WORD_COOCS]:\n",
    "        G.add_edge(word1, word2, weight=count)\n",
    "        \n",
    "    T = nx.minimum_spanning_tree(G)\n",
    "\n",
    "    return T\n",
    "\n",
    "\n",
    "def get_sorted_word_coocs(word_cooc_mat):\n",
    "    \"\"\"주어진 어휘 공기 행렬에서 공기 빈도로 역술 정렬된 행렬을 생성하려 돌려준다.\"\"\"\n",
    "    \n",
    "    print_log_msg(\"Sorting topic word occurrence.\")\n",
    "    word_coocs = []\n",
    "    \n",
    "    for word1, word2_counter in word_cooc_mat.items():\n",
    "        for word2, count in word2_counter.items():\n",
    "            word_coocs.append((word1, word2, count))\n",
    "            \n",
    "    sorted_word_coocs = sorted(word_coocs, key=itemgetter(2), reverse=True)\n",
    "    \n",
    "    return sorted_word_coocs\n",
    "\n",
    "\n",
    "def draw_network(G):\n",
    "    \"\"\"주어진 어휘 공기 네트워크를 화면에 표시한다.\"\"\"\n",
    "    \n",
    "    print_log_msg(\"Drawing topic word network.\")\n",
    "    font_name = get_font_name()\n",
    "    nx.draw_networkx(G,\n",
    "            pos=nx.spring_layout(G, k=0.8, scale=2),\n",
    "            node_size=800,\n",
    "            node_color=\"yellow\",\n",
    "            font_family=font_name,\n",
    "            label_pos=0,  # 0=head, 0.5=center, 1=tail\n",
    "            with_labels=True,\n",
    "            font_size=10)\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    # plt.savefig(\"graph.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def get_font_name():\n",
    "    \"\"\"플랫폼에 따라 화면에 표시할 글꼴 이름을 돌려준다.\"\"\"\n",
    "    \n",
    "    if sys.platform in [\"win32\", \"win64\"]:\n",
    "        font_name = \"malgun gothic\"\n",
    "    elif sys.platform == \"darwin\":\n",
    "        font_name = \"AppleGothic\"\n",
    "        \n",
    "    return font_name\n",
    "\n",
    "\n",
    "def print_document_topics(model, corpus):\n",
    "    \"\"\"주어진 토픽 모델링 결과와 문서 어휘 행렬에서 문서별 토픽 분포를 출력한다.\"\"\"\n",
    "    \n",
    "    for doc_num, doc in enumerate(corpus):\n",
    "        topic_probs = model[doc]\n",
    "        print(\"Doc num: {}\".format(doc_num))\n",
    "\n",
    "        for topic_id, prob in topic_probs:\n",
    "            print(\"\\t{}\\t{}\".format(topic_id, prob))\n",
    "            \n",
    "        break\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "def print_log_msg(msg):\n",
    "    \"\"\"로그 메시지를 출력한다.\"\"\"\n",
    "    \n",
    "    print(msg, flush=True)\n",
    "        \n",
    "        \n",
    "\n",
    "input_file_name = \"data/blog_text_국립중앙박물관(1-500).txt\"\n",
    "documents = read_documents(input_file_name)\n",
    "tokenized_documents = text_tokenizing(documents, tokenizer=\"noun\") #tokenizer= \"noun\" or \"word\"\n",
    "corpus, dictionary = build_doc_term_mat(tokenized_documents)\n",
    "model = models.ldamodel.LdaModel(corpus, num_topics=NUM_TOPICS,\n",
    "                                 id2word=dictionary,\n",
    "                                 alpha=1)\n",
    "print_document_topics(model, corpus)\n",
    "word_cooc_mat = build_word_cooc_mat(model)\n",
    "sorted_word_coocs = get_sorted_word_coocs(word_cooc_mat)\n",
    "G = build_word_cooc_network(sorted_word_coocs)\n",
    "draw_network(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "data = pyLDAvis.gensim.prepare(model, corpus, dictionary)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lda2vec](figs/lda2vec.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_metadata": {
   "author": "이기황",
   "coursetitle": "텍스트분석기법",
   "courseyear": "2018",
   "date": "2018.04.18",
   "logofile": "figs/ewhauniv-logo.png",
   "logoraise": "-.2",
   "logoscale": ".4",
   "title": "단어 임베딩과 토픽 모델링"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
